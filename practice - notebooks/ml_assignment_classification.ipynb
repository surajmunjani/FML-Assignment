{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "193dd0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef1c7787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The seed will be fixed to 42 for this assigmnet.\n",
    "np.random.seed(42)\n",
    "# \n",
    "NUM_FEATS = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "960a53a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(object):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_layers, num_units, num_classes):\n",
    "        '''\n",
    "        Initialize the neural network.\n",
    "        Create weights and biases.\n",
    "        Here, we have provided an example structure for the weights and biases.\n",
    "        It is a list of weight and bias matrices, in which, the\n",
    "        dimensions of weights and biases are (assuming 1 input layer, 2 hidden layers, and 1 output layer):\n",
    "        weights: [(NUM_FEATS, num_units), (num_units, num_units), (num_units, num_units), (num_units, 1)]\n",
    "        biases: [(num_units, 1), (num_units, 1), (num_units, 1), (num_units, 1)]\n",
    "        Please note that this is just an example.\n",
    "        You are free to modify or entirely ignore this initialization as per your need.\n",
    "        Also you can add more state-tracking variables that might be useful to compute\n",
    "        the gradients efficiently.\n",
    "        Parameters\n",
    "        ----------\n",
    "            num_layers : Number of HIDDEN layers.\n",
    "            num_units : Number of units in each Hidden layer.\n",
    "        '''\n",
    "        if len(num_units)!=num_layers:\n",
    "            raise Exception('Number of Hidden Layers and list of neurons are not compatible')\n",
    "        self.num_layers = num_layers\n",
    "        self.num_units = num_units\n",
    "        self.num_classes = num_classes\n",
    "        self.biases = []\n",
    "        self.weights = []\n",
    "        for i in range(num_layers):\n",
    "            if i==0:\n",
    "                # Input layer\n",
    "                self.weights.append(np.random.uniform(-1, 1, size=(NUM_FEATS, self.num_units[i])))\n",
    "            else:\n",
    "                # Hidden layer\n",
    "                self.weights.append(np.random.uniform(-1, 1, size=(self.num_units[i-1], self.num_units[i])))\n",
    "            self.biases.append(np.random.uniform(-1, 1, size=(self.num_units[i], 1)))\n",
    "\n",
    "        # Output layer\n",
    "        self.biases.append(np.random.uniform(-1, 1, size=(self.num_classes, 1)))\n",
    "        self.weights.append(np.random.uniform(-1, 1, size=(self.num_units[-1], self.num_classes)))\n",
    "\n",
    "        self.h_states = []\n",
    "        self.a_states = []\n",
    "        self.pred = 0\n",
    "\n",
    "    def __call__(self, X, activation_fn):\n",
    "        '''\n",
    "        Forward propagate the input X through the network,\n",
    "        and return the output.\n",
    "        Note that for a classification task, the output layer should\n",
    "        be a softmax layer. So perform the computations accordingly\n",
    "        Parameters\n",
    "        ----------\n",
    "            X : Input to the network, numpy array of shape m x d\n",
    "        Returns\n",
    "        ----------\n",
    "            y : Output of the network, numpy array of shape m x num_classes\n",
    "        '''\n",
    "        self.h_states = []\n",
    "        self.a_states = []\n",
    "        a, h = X, X\n",
    "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            self.h_states.append(h)\n",
    "            self.a_states.append(a)\n",
    "\n",
    "            h = np.dot(a, w) + b.T\n",
    "            a = softmax(h) if i==len(self.weights)-1 else activation_fn(h)\n",
    "\n",
    "        self.h_states.append(h)\n",
    "        self.a_states.append(a)\n",
    "\n",
    "        self.pred = a\n",
    "        return self.pred\n",
    "#         raise NotImplementedError\n",
    "\n",
    "    def backward(self, X, y, batch_size, activation_fn, lamda):\n",
    "        '''\n",
    "        Compute and return gradients loss with respect to weights and biases.\n",
    "        (dL/dW and dL/db)\n",
    "        Parameters\n",
    "        ----------\n",
    "            X : Input to the network, numpy array of shape m x d\n",
    "            y : Output of the network, numpy array of shape m x num_classes\n",
    "            lamda : Regularization parameter.\n",
    "        Returns\n",
    "        ----------\n",
    "            del_W : derivative of loss w.r.t. all weight values (a list of matrices).\n",
    "            del_b : derivative of loss w.r.t. all bias values (a list of vectors).\n",
    "        Hint: You need to do a forward pass before performing backward pass.\n",
    "        '''\n",
    "        del_W = []\n",
    "        del_b = []\n",
    "        \n",
    "        d_w_layer = self.a_states[-1] - y\n",
    "#         d_w_layer = (-y/self.a_states[-1]+(1-y)/(1-self.a_states[-1]))*stable_softmax(self.h_states[-1],derivative=True)\n",
    "    \n",
    "        del_W_n = 1./batch_size*np.dot(self.a_states[-2].T,d_w_layer) + 2*lamda*self.weights[-1]\n",
    "        del_b_n = 1./batch_size*np.sum(np.dot(self.a_states[-2].T,d_w_layer), axis=0).reshape(-1,1) + 2*lamda*self.biases[-1]\n",
    "        \n",
    "        d_b_layer = del_b_n\n",
    "        \n",
    "        del_W.insert(0, del_W_n)\n",
    "        del_b.insert(0, del_b_n)\n",
    "        \n",
    "        for i in range(self.num_layers, 0, -1):\n",
    "            # with respect to inactivated neuron\n",
    "            # d_w_layer = np.multiply(np.dot(d_w_layer,self.weights[i].T), activation_fn(self.h_states[i], derivative=True))\n",
    "            # d_b_layer = np.multiply(np.dot(d_b_layer,self.weights[i].T), activation_fn(self.h_states[i], derivative=True))\n",
    "            # d_b_layer = d_w_layer\n",
    "\n",
    "            # with respect to activated neuron\n",
    "            if i==self.num_layers:\n",
    "                # d_w_layer = np.multiply(np.dot(np.multiply(d_w_layer,stable_softmax(self.h_states[i+1], derivative=True)),self.weights[i].T), activation_fn(self.h_states[i], derivative=True))\n",
    "                # d_b_layer = np.multiply(np.dot(np.multiply(d_b_layer.T,stable_softmax(self.h_states[i+1], derivative=True)),self.weights[i].T), activation_fn(self.h_states[i], derivative=True))\n",
    "                d_w_layer = np.multiply(np.dot(d_w_layer,self.weights[i].T), activation_fn(self.h_states[i], derivative=True))\n",
    "                d_b_layer = np.multiply(np.dot(d_b_layer.T,self.weights[i].T), activation_fn(self.h_states[i], derivative=True))\n",
    "            else:\n",
    "                d_w_layer = np.multiply(np.dot(np.multiply(d_w_layer,activation_fn(self.h_states[i+1], derivative=True)),self.weights[i].T), activation_fn(self.h_states[i], derivative=True))\n",
    "                d_b_layer = np.multiply(np.dot(np.multiply(d_b_layer,activation_fn(self.h_states[i+1], derivative=True)),self.weights[i].T), activation_fn(self.h_states[i], derivative=True))    \n",
    "            \n",
    "            del_W_i = np.dot(self.a_states[i-1].T,d_w_layer) + 2*lamda*self.weights[i-1]\n",
    "            del_b_i = np.sum(d_b_layer, axis=0).reshape(len(self.biases[i-1]),1) + 2*lamda*self.biases[i-1]\n",
    "\n",
    "            del_W.insert(0,del_W_i)\n",
    "            del_b.insert(0,del_b_i)\n",
    "        \n",
    "        return del_W, del_b\n",
    "#         raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de7ec138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(h, derivative=False):\n",
    "    h = 1/(1+np.exp(-h))\n",
    "    if derivative:\n",
    "        h = h*(1-h)\n",
    "    return h\n",
    "                                       \n",
    "def relu(h, derivative=False):        \n",
    "    if derivative:\n",
    "        return np.heaviside(h ,0)\n",
    "    return np.maximum(0, h)\n",
    "\n",
    "def leaky_relu(h, alpha=0.1, derivative=False):\n",
    "    if derivative:\n",
    "        return np.where(h>0, 1, alpha)\n",
    "    return np.where(h>0, h, h*alpha)\n",
    "\n",
    "def softmax(h, derivative=False):\n",
    "    h = np.exp(h)/np.sum(np.exp(h), axis=1).reshape(h.shape[0],1)\n",
    "    if derivative:\n",
    "        return h*(1-h)\n",
    "    return h\n",
    "\n",
    "def stable_softmax(h, derivative=False):\n",
    "    h = np.exp(h- h.max())/np.sum(np.exp(h-h.max()), axis=1).reshape(h.shape[0],1)\n",
    "    if derivative:\n",
    "        return h*(1-h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb9e5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    def __init__(self, learning_rate, optimization, beta=0.9, gamma=0.999):\n",
    "        '''\n",
    "        Create a Gradient Descent based optimizer with given\n",
    "        learning rate.\n",
    "        Other parameters can also be passed to create different types of\n",
    "        optimizers.\n",
    "        Hint: You can use the class members to track various states of the\n",
    "        optimizer.\n",
    "        '''\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1e-6\n",
    "        self.t = 0\n",
    "        self.w_states = []\n",
    "        self.b_states = []\n",
    "        self.w_momentum = []\n",
    "        self.b_momentum = []\n",
    "        self.optimization_algorithm = optimization\n",
    "#         raise NotImplementedError\n",
    "\n",
    "    def step(self, weights, biases, delta_weights, delta_biases):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "            weights: Current weights of the network.\n",
    "            biases: Current biases of the network.\n",
    "            delta_weights: Gradients of weights with respect to loss.\n",
    "            delta_biases: Gradients of biases with respect to loss.\n",
    "        '''\n",
    "        \n",
    "        if self.optimization_algorithm == 'SGD':      \n",
    "            self.t+=1\n",
    "            for i in range(len(weights)):\n",
    "                weights[i] = weights[i] - self.learning_rate*delta_weights[i]\n",
    "                biases[i] = biases[i] - self.learning_rate*delta_biases[i]\n",
    "        \n",
    "        if self.optimization_algorithm == 'RMSProp':\n",
    "            if self.t == 0:\n",
    "                for i in range(len(weights)):\n",
    "                    a,b = weights[i].shape\n",
    "                    self.w_states.append(np.zeros([a,b]))\n",
    "                    a,b = biases[i].shape\n",
    "                    self.b_states.append(np.zeros([a,b]))\n",
    "            \n",
    "            self.t+=1\n",
    "            for i in range(len(weights)):\n",
    "                self.w_states[i] = self.gamma*self.w_states[i] + (1-self.gamma)*np.multiply(delta_weights[i],delta_weights[i])\n",
    "                weights[i] = weights[i] - (self.learning_rate/(np.sqrt(self.w_states[i])+self.epsilon))*delta_weights[i]\n",
    "                self.b_states[i] = self.gamma*self.b_states[i] + (1-self.gamma)*np.multiply(delta_biases[i],delta_biases[i])\n",
    "                biases[i] = biases[i] - (self.learning_rate/(np.sqrt(self.b_states[i])+self.epsilon))*delta_biases[i]\n",
    "\n",
    "        \n",
    "        if self.optimization_algorithm == 'Adam':\n",
    "            if self.t == 0:\n",
    "                for i in range(len(weights)):\n",
    "                    a,b = weights[i].shape\n",
    "                    self.w_states.append(np.zeros([a,b]))\n",
    "                    self.w_momentum.append(np.zeros([a,b]))\n",
    "                    a,b = biases[i].shape\n",
    "                    self.b_states.append(np.zeros([a,b]))\n",
    "                    self.b_momentum.append(np.zeros([a,b]))\n",
    "            \n",
    "            self.t+=1\n",
    "            for i in range(len(weights)):\n",
    "                self.w_momentum[i] = self.beta*self.w_momentum[i] + (1-self.beta)*delta_weights[i]\n",
    "                self.w_states[i] = self.gamma*self.w_states[i] + (1-self.gamma)*np.multiply(delta_weights[i],delta_weights[i])\n",
    "                weights[i] = weights[i] - (self.learning_rate/(np.sqrt(self.w_states[i]/(1-self.gamma**self.t))+self.epsilon))*(self.w_momentum[i]/(1-self.beta**self.t))\n",
    "                self.b_momentum[i] = self.beta*self.b_momentum[i] + (1-self.beta)*delta_biases[i]\n",
    "                self.b_states[i] = self.gamma*self.b_states[i] + (1-self.gamma)*np.multiply(delta_biases[i],delta_biases[i])\n",
    "                biases[i] = biases[i] - (self.learning_rate/(np.sqrt(self.b_states[i]/(1-self.gamma**self.t))+self.epsilon))*(self.b_momentum[i]/(1-self.beta**self.t))\n",
    "\n",
    "        return weights, biases\n",
    "#         raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f541b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y, y_hat):\n",
    "    '''\n",
    "    Compute cross entropy loss\n",
    "    Parameters\n",
    "    ----------\n",
    "        y : targets, numpy array of shape m x 4\n",
    "        y_hat : predictions, numpy array of shape m x 4\n",
    "    Returns\n",
    "    ----------\n",
    "        cross entropy loss\n",
    "    '''\n",
    "    return -np.sum(np.multiply(y,np.log(y_hat)))\n",
    "#     raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba4dda64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    net, optimizer, lamda, batch_size, max_epochs,\n",
    "    train_input, train_target,\n",
    "    dev_input, dev_target, activation_fn\n",
    "):\n",
    "    '''\n",
    "    In this function, you will perform following steps:\n",
    "        1. Run gradient descent algorithm for `max_epochs` epochs.\n",
    "        2. For each bach of the training data\n",
    "            1.1 Compute gradients\n",
    "            1.2 Update weights and biases using step() of optimizer.\n",
    "        3. Compute RMSE on dev data after running `max_epochs` epochs.\n",
    "    Here we have added the code to loop over batches and perform backward pass\n",
    "    for each batch in the loop.\n",
    "    For this code also, you are free to heavily modify it.\n",
    "    '''\n",
    "\n",
    "    m = train_input.shape[0]\n",
    "    dev_target = np.array([get_one_hot_encoding_of_label(x,one_hot_encoding) for x in dev_target.to_numpy()])\n",
    "    for e in range(max_epochs):\n",
    "        epoch_loss = 0.\n",
    "        number_of_batches = 0\n",
    "        for i in range(0, m, batch_size):\n",
    "            batch_input = train_input[i:i+batch_size]\n",
    "#             batch_target = train_target[i:i+batch_size]\n",
    "#             batch_target = np.array(train_target[i:i+batch_size]).astype(float).reshape(len(batch_input),1)\n",
    "            batch_target = np.array([get_one_hot_encoding_of_label(x,one_hot_encoding) for x in train_target[i:i+batch_size].to_numpy()])\n",
    "            pred = net(batch_input, activation_fn)\n",
    "\n",
    "            # Compute gradients of loss w.r.t. weights and biases\n",
    "            dW, db = net.backward(batch_input, batch_target, batch_size, activation_fn, lamda)\n",
    "\n",
    "            # Get updated weights based on current weights and gradients\n",
    "            weights_updated, biases_updated = optimizer.step(net.weights, net.biases, dW, db)\n",
    "#             net.weights, net.biases = optimizer.step(net.weights, net.biases, dW, db)\n",
    "\n",
    "            # Update model's weights and biases\n",
    "            net.weights = weights_updated\n",
    "            net.biases = biases_updated\n",
    "\n",
    "            # Compute loss for the batch\n",
    "            batch_loss = cross_entropy_loss(batch_target, pred)\n",
    "            epoch_loss += batch_loss\n",
    "            number_of_batches += 1\n",
    "#             print('Epoch: {}, iteration: {}, batch_loss: {}'.format(e, i, batch_loss))\n",
    "#         epoch.append(e)\n",
    "#         epoch_loss = epoch_loss/number_of_batches\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        dev_loss=cross_entropy_loss(dev_target, net(dev_input, activation_fn))\n",
    "        dev_losses.append(dev_loss)\n",
    "        print('Epoch: {}, Dev Loss: {}, Epoch Loss: {}'.format(e, dev_loss, epoch_loss))\n",
    "\n",
    "        # Write any early stopping conditions required (only for Part 2)\n",
    "        # Hint: You can also compute dev_rmse here and use it in the early\n",
    "        #       stopping condition.\n",
    "\n",
    "    # After running `max_epochs` (for Part 1) epochs OR early stopping (for Part 2), compute the RMSE on dev data.\n",
    "    \n",
    "    dev_pred = net(dev_input, activation_fn)\n",
    "    dev_celoss = cross_entropy_loss(dev_target, dev_pred)\n",
    "\n",
    "    print('CE on dev data: {:.5f}'.format(dev_celoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e209abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_predictions(net, inputs, activation_fn):\n",
    "    '''\n",
    "    Perform forward pass on test data and get the final predictions that can\n",
    "    be submitted on Kaggle.\n",
    "    Write the final predictions to the part2.csv file.\n",
    "    Parameters\n",
    "    ----------\n",
    "        net : trained neural network\n",
    "        inputs : test input, numpy array of shape m x d\n",
    "    Returns\n",
    "    ----------\n",
    "        predictions (optional): Predictions obtained from forward pass\n",
    "                                on test data, numpy array of shape m x 1\n",
    "    '''\n",
    "    return net(inputs, activation_fn)\n",
    "#     raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9652312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_classification_data():\n",
    "    '''\n",
    "    Read the train, dev, and test datasets\n",
    "    '''\n",
    "    \n",
    "    train_input = pd.read_csv(\"https://raw.githubusercontent.com/sahasrarjn/cs725-2022-assignment/main/classification/data/train.csv\")\n",
    "#     classes = set(train_input.iloc[:,0])\n",
    "#     one_hot_encoding = {}\n",
    "#     for i,label in enumerate(classes):\n",
    "#         one_hot_encoding[label] = i\n",
    "#     train_input.iloc[:,0] = train_input.iloc[:,0].apply(lambda x: get_one_hot_encoding_of_label(x, one_hot_encoding))\n",
    "    train_target = train_input.iloc[:,0]\n",
    "    train_input.drop(columns=train_input.columns[0],\n",
    "                 axis=0,\n",
    "                 inplace=True)\n",
    "    \n",
    "    dev_input = pd.read_csv(\"https://raw.githubusercontent.com/sahasrarjn/cs725-2022-assignment/main/classification/data/dev.csv\")\n",
    "#     dev_input.iloc[:,0] = dev_input.iloc[:,0].apply(lambda x: get_one_hot_encoding_of_label(x, one_hot_encoding))\n",
    "    dev_target = dev_input.iloc[:,0]\n",
    "    dev_input.drop(columns=dev_input.columns[0],\n",
    "                 axis=0,\n",
    "                 inplace=True)\n",
    "    \n",
    "    test_input = pd.read_csv(\"https://raw.githubusercontent.com/sahasrarjn/cs725-2022-assignment/main/classification/data/test.csv\")\n",
    "    return train_input, train_target, dev_input, dev_target, test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05b089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # Hyper-parameters \n",
    "    max_epochs = 50\n",
    "    batch_size = 256\n",
    "    learning_rate = 0.001\n",
    "    num_layers = 1\n",
    "    num_units = 64\n",
    "    lamda = 0.1 # Regularization Parameter\n",
    "\n",
    "    train_input, train_target, dev_input, dev_target, test_input = read_data()\n",
    "    net = Net(num_layers, num_units)\n",
    "    optimizer = Optimizer(learning_rate)\n",
    "    train(\n",
    "        net, optimizer, lamda, batch_size, max_epochs,\n",
    "        train_input, train_target,\n",
    "        dev_input, dev_target\n",
    "    )\n",
    "#     get_test_data_predictions(net, test_input)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94dc472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(input_df, method='min_max_normalization', rescaling_range = (-1,1)):\n",
    "    for col in input_df:\n",
    "        mean = input_df[col].mean()\n",
    "        std = input_df[col].std()\n",
    "        min_value = input_df[col].min()\n",
    "        max_value = input_df[col].max()\n",
    "        if method=='min_max_normalization':\n",
    "            input_df[col] = (input_df[col]-min_value)/(max_value - min_value)\n",
    "        elif method=='min_max_normalization_with_rescaling':\n",
    "            input_df[col] = rescaling_range[0] + ((input_df[col]-min_value)*(rescaling_range[1]-rescaling_range[0])/(max_value - min_value))\n",
    "        elif method=='mean_normalization':\n",
    "            input_df[col] = (input_df[col]-mean)/(max_value - min_value)\n",
    "        elif method=='z_score_normalization':\n",
    "            input_df[col] = (input_df[col]-mean)/std\n",
    "        else:\n",
    "            raise Exception('no such normalization method implemented')\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59aac584",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_train_input, classification_train_target, classification_dev_input, classification_dev_target, classification_test_input = read_classification_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "052bbac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_train_input = feature_scaling(classification_train_input)\n",
    "# classification_dev_input = feature_scaling(classification_dev_input)\n",
    "# classification_test_input = feature_scaling(classification_test_input)\n",
    "\n",
    "classification_train_input = feature_scaling(classification_train_input, method='z_score_normalization')\n",
    "classification_dev_input = feature_scaling(classification_dev_input, method='z_score_normalization')\n",
    "classification_test_input = feature_scaling(classification_test_input, method='z_score_normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cab51164",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = set(classification_train_target)\n",
    "one_hot_encoding = {}\n",
    "inverse_one_hot = {}\n",
    "for i,label in enumerate(classes):\n",
    "    one_hot_encoding[label] = i\n",
    "    inverse_one_hot[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5720a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_encoding_of_label(label, one_hot_encoding):\n",
    "    encoding = np.zeros(len(one_hot_encoding))\n",
    "    encoding[one_hot_encoding[label]]=1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c55ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_scaling(input_series, method='min_max_normalization', rescaling_range=(-1,1)):\n",
    "    mean = input_series.mean()\n",
    "    std = input_series.std()\n",
    "    min_value = input_series.min()\n",
    "    max_value = input_series.max()\n",
    "    if method=='min_max_normalization':\n",
    "        input_series = (input_series-min_value)/(max_value-min_value)\n",
    "    elif method=='min_max_normalization_with_rescaling':\n",
    "        input_series = rescaling_range[0] + ((input_series-min_value)*(rescaling_range[1]-rescaling_range[0])/(max_value - min_value))\n",
    "    elif method=='mean_normalization':\n",
    "        input_series = (input_series-mean)/(max_value - min_value)\n",
    "    elif method=='z_score_normalization':\n",
    "        input_series = (input_series-mean)/std\n",
    "    else:\n",
    "        raise Exception('no such normalization method implemented')\n",
    "    return mean, std, min_value, max_value, method, rescaling_range, input_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1217e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_rescaling(input_series, mean, std, min_value, max_value, method, rescaling_range):\n",
    "    if method=='min_max_normalization':\n",
    "        input_series = input_series*(max_value-min_value)+min_value\n",
    "    elif method=='min_max_normalization_with_rescaling':\n",
    "        input_series = ((input_series-rescaling_range[0])*(max_value-min_value))/(rescaling_range[1]-rescaling_range[0])+min_value\n",
    "    elif method=='mean_normalization':\n",
    "        input_series = input_series*(max_value-min_value)+mean\n",
    "    elif method=='z_score_normalization':\n",
    "        input_series = input_series*std+mean\n",
    "    else:\n",
    "        raise Exception('no such normalization method implemented')\n",
    "    return input_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1c86ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "num_units = [75]\n",
    "num_classes = len(classes)\n",
    "\n",
    "# df = pd.DataFrame({\n",
    "#     'Name': ['learning_rate', 'batch_size', 'num_layers', 'num_units', 'regularization_parameter'],\n",
    "#     'Value': [learning_rate, batch_size, num_layers, num_units, lamda]\n",
    "# })\n",
    "# # df.to_csv('./IITB/CS725/assignment/cs725-2022-assignment-regression/part_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9066092",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(num_layers, num_units, num_classes)\n",
    "epoch = []\n",
    "epoch_losses = []\n",
    "dev_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf4aedb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Dev Loss: 41552.65462554608, Epoch Loss: 391366.5443278112\n",
      "Epoch: 1, Dev Loss: 35628.3784666244, Epoch Loss: 308961.82596545346\n",
      "Epoch: 2, Dev Loss: 31875.301928963592, Epoch Loss: 270945.84491223405\n",
      "Epoch: 3, Dev Loss: 29146.631255440087, Epoch Loss: 244711.8806871389\n",
      "Epoch: 4, Dev Loss: 27049.46730111733, Epoch Loss: 224854.85556206544\n",
      "Epoch: 5, Dev Loss: 25360.349652233228, Epoch Loss: 209055.5170532722\n",
      "Epoch: 6, Dev Loss: 23955.612701225473, Epoch Loss: 196001.5927513071\n",
      "Epoch: 7, Dev Loss: 22752.05017361598, Epoch Loss: 184942.89719551706\n",
      "Epoch: 8, Dev Loss: 21698.806469362447, Epoch Loss: 175381.30057154\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Optimizer(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, optimization\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSProp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# optimizer = Optimizer(learning_rate=learning_rate, optimization='Adam')\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlamda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassification_train_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassification_train_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassification_dev_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassification_dev_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelu\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, optimizer, lamda, batch_size, max_epochs, train_input, train_target, dev_input, dev_target, activation_fn)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#             batch_target = train_target[i:i+batch_size]\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#             batch_target = np.array(train_target[i:i+batch_size]).astype(float).reshape(len(batch_input),1)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m             batch_target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([get_one_hot_encoding_of_label(x,one_hot_encoding) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m train_target[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\u001b[38;5;241m.\u001b[39mto_numpy()])\n\u001b[0;32m---> 28\u001b[0m             pred \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;66;03m# Compute gradients of loss w.r.t. weights and biases\u001b[39;00m\n\u001b[1;32m     31\u001b[0m             dW, db \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mbackward(batch_input, batch_target, batch_size, activation_fn, lamda)\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mNet.__call__\u001b[0;34m(self, X, activation_fn)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_states\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_states\u001b[38;5;241m.\u001b[39mappend(a)\n\u001b[0;32m---> 67\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\n\u001b[1;32m     68\u001b[0m     a \u001b[38;5;241m=\u001b[39m softmax(h) \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m activation_fn(h)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_states\u001b[38;5;241m.\u001b[39mappend(h)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "learning_rate=1e-4\n",
    "max_epochs=50\n",
    "lamda=0\n",
    "optimizer = Optimizer(learning_rate=learning_rate, optimization='RMSProp')\n",
    "# optimizer = Optimizer(learning_rate=learning_rate, optimization='Adam')\n",
    "train(\n",
    "    net, optimizer, lamda, batch_size, max_epochs,\n",
    "    classification_train_input, classification_train_target,\n",
    "    classification_dev_input, classification_dev_target, relu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee87b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-4\n",
    "max_epochs=50\n",
    "lamda=0\n",
    "# optimizer = Optimizer(learning_rate=learning_rate, optimization='RMSProp')\n",
    "optimizer = Optimizer(learning_rate=learning_rate, optimization='Adam')\n",
    "train(\n",
    "    net, optimizer, lamda, batch_size, max_epochs,\n",
    "    classification_train_input, classification_train_target,\n",
    "    classification_dev_input, classification_dev_target, relu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f408bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-4\n",
    "max_epochs=50\n",
    "lamda=0\n",
    "# optimizer = Optimizer(learning_rate=learning_rate, optimization='Adam')\n",
    "optimizer = Optimizer(learning_rate=learning_rate, optimization='RMSProp')\n",
    "train(\n",
    "    net, optimizer, lamda, batch_size, max_epochs,\n",
    "    classification_train_input, classification_train_target,\n",
    "    classification_dev_input, classification_dev_target, relu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-4\n",
    "max_epochs=100\n",
    "lamda=0\n",
    "# optimizer = Optimizer(learning_rate=learning_rate, optimization='Adam')\n",
    "optimizer = Optimizer(learning_rate=learning_rate, optimization='RMSProp')\n",
    "train(\n",
    "    net, optimizer, lamda, batch_size, max_epochs,\n",
    "    classification_train_input, classification_train_target,\n",
    "    classification_dev_input, classification_dev_target, relu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deffa8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-4\n",
    "max_epochs=50\n",
    "lamda=0\n",
    "optimizer = Optimizer(learning_rate=learning_rate, optimization='Adam')\n",
    "# optimizer = Optimizer(learning_rate=learning_rate, optimization='RMSProp')\n",
    "train(\n",
    "    net, optimizer, lamda, batch_size, max_epochs,\n",
    "    classification_train_input, classification_train_target,\n",
    "    classification_dev_input, classification_dev_target, relu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-4\n",
    "max_epochs=50\n",
    "lamda=0\n",
    "# optimizer = Optimizer(learning_rate=learning_rate, optimization='Adam')\n",
    "optimizer = Optimizer(learning_rate=learning_rate, optimization='RMSProp')\n",
    "train(\n",
    "    net, optimizer, lamda, batch_size, max_epochs,\n",
    "    classification_train_input, classification_train_target,\n",
    "    classification_dev_input, classification_dev_target, relu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-4\n",
    "max_epochs=50\n",
    "lamda=0\n",
    "# optimizer = Optimizer(learning_rate=learning_rate, optimization='Adam')\n",
    "optimizer = Optimizer(learning_rate=learning_rate, optimization='RMSProp')\n",
    "train(\n",
    "    net, optimizer, lamda, batch_size, max_epochs,\n",
    "    classification_train_input, classification_train_target,\n",
    "    classification_dev_input, classification_dev_target, relu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44593cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-4\n",
    "max_epochs=50\n",
    "lamda=0\n",
    "# optimizer = Optimizer(learning_rate=learning_rate, optimization='Adam')\n",
    "optimizer = Optimizer(learning_rate=learning_rate, optimization='RMSProp')\n",
    "train(\n",
    "    net, optimizer, lamda, batch_size, max_epochs,\n",
    "    classification_train_input, classification_train_target,\n",
    "    classification_dev_input, classification_dev_target, relu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93f8d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-4\n",
    "max_epochs=100\n",
    "lamda=0\n",
    "# optimizer = Optimizer(learning_rate=learning_rate, optimization='Adam')\n",
    "optimizer = Optimizer(learning_rate=learning_rate, optimization='RMSProp')\n",
    "train(\n",
    "    net, optimizer, lamda, batch_size, max_epochs,\n",
    "    classification_train_input, classification_train_target,\n",
    "    classification_dev_input, classification_dev_target, relu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257256d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6006baa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0060dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97824a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_target = np.array(dev_target).astype(float).reshape(len(dev_target),1)\n",
    "dev_target = np.array([get_one_hot_encoding_of_label(x,one_hot_encoding) for x in classification_dev_target.to_numpy()])\n",
    "dev_pred = net(classification_dev_input, relu)\n",
    "# dev_rmse = rmse(dev_target_scaled, dev_pred)\n",
    "dev_rmse = cross_entropy_loss(dev_target, dev_pred)\n",
    "\n",
    "print('CE on dev data: {:.5f}'.format(dev_rmse))\n",
    "# print('RMSE on dev data: {:.5f}'.format(dev_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea0bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = dev_pred.argmax(axis=1)\n",
    "pred = []\n",
    "for i, row in enumerate(dev_pred.argmax(axis=1)):\n",
    "    pred.append(inverse_one_hot[indices[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1261c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "match = 0\n",
    "for y_hat, y in zip(pred,classification_dev_target.to_list()):\n",
    "    if y_hat==y:\n",
    "        match+=1\n",
    "print('Accuracy Score: {}'.format(match/len(dev_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c476081",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_test_input = pd.read_csv('/Users/manasgabani/Downloads/IITB/CS725/assignment/cs-725-autumn-2022-assignment-classification/test.csv')\n",
    "test_pred = net(classification_test_input, sigmoid)\n",
    "indices = test_pred.argmax(axis=1)\n",
    "test_label_pred = []\n",
    "for i, row in enumerate(test_pred.argmax(axis=1)):\n",
    "    test_label_pred.append(inverse_one_hot[indices[i]])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Id': list(classification_test_input.index+1),\n",
    "    'Predictions': test_label_pred\n",
    "})\n",
    "\n",
    "# df.to_csv('./IITB/CS725/assignment/cs725-2022-assignment-regression/22M0781.csv',index=False)\n",
    "df.to_csv('../Documents/22M0781.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c462409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X , num_components):\n",
    "     \n",
    "    #Step-1\n",
    "    X_meaned = X - np.mean(X , axis = 0)\n",
    "     \n",
    "    #Step-2\n",
    "    cov_mat = np.cov(X_meaned , rowvar = False)\n",
    "     \n",
    "    #Step-3\n",
    "    eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n",
    "     \n",
    "    #Step-4\n",
    "    sorted_index = np.argsort(eigen_values)[::-1]\n",
    "    sorted_eigenvalue = eigen_values[sorted_index]\n",
    "    sorted_eigenvectors = eigen_vectors[:,sorted_index]\n",
    "     \n",
    "    #Step-5\n",
    "    eigenvector_subset = sorted_eigenvectors[:,0:num_components]\n",
    "     \n",
    "    #Step-6\n",
    "    X_reduced = np.dot(eigenvector_subset.transpose() , X_meaned.transpose() ).transpose()\n",
    "     \n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a3b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA(classification_train_input,90).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57132c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step-1\n",
    "X_meaned = classification_train_input - np.mean(classification_train_input , axis = 0)\n",
    "     \n",
    "#Step-2\n",
    "cov_mat = np.cov(X_meaned , rowvar = False)\n",
    "\n",
    "#Step-3\n",
    "eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n",
    "\n",
    "#Step-4\n",
    "sorted_index = np.argsort(eigen_values)[::-1]\n",
    "sorted_eigenvalue = eigen_values[sorted_index]\n",
    "sorted_eigenvectors = eigen_vectors[:,sorted_index]\n",
    "\n",
    "#Step-5\n",
    "# eigenvector_subset = sorted_eigenvectors[:,0:num_components]\n",
    "\n",
    "#Step-6\n",
    "# X_reduced = np.dot(eigenvector_subset.transpose() , X_meaned.transpose() ).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_eigenvectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b471d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c87a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_test_input = pd.read_csv('/Users/manasgabani/Downloads/IITB/CS725/assignment/cs-725-autumn-2022-assignment-classification/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa13fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(50,100):\n",
    "#     epoch[i] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c70958",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(epoch_losses, label='Epoch Loss', ls='-', color='red', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Epoch loss')\n",
    "plt.grid(visible='on')\n",
    "plt.legend(loc=0)\n",
    "# plt.savefig('./IITB/CS725/assignment/cs725-2022-assignment-regression/train_64.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee06eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(dev_losses, label='dev loss', ls='-', color='blue', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('dev loss')\n",
    "plt.grid(visible='on')\n",
    "plt.legend(loc=0)\n",
    "# plt.savefig('./IITB/CS725/assignment/cs725-2022-assignment-regression/dev_64.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb22b21c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ddcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "net(dev_input, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ddc1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net(test_input, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2266705",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_target.mean(), train_target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f69252",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_W, del_b = net.backward(dev_input, pred, len(dev_input), relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7431f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "net(dev_input).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8beb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, dev_input, dev_target, test_input = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480440b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input=pd.read_csv(\"https://raw.githubusercontent.com/sahasrarjn/cs725-2022-assignment/main/regression/data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a18161",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f4e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input.drop(columns=train_input.columns[0],\n",
    "                 axis=0,\n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc64117",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b4373",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_target[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.del_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58380bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 3\n",
    "num_units = 64\n",
    "net = Net(num_layers, num_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c64127",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(train_target[:32]).astype(float).reshape(32,1)\n",
    "x = train_input[0:32]\n",
    "pred = net(x)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c66796",
   "metadata": {},
   "outputs": [],
   "source": [
    "back = net.backward(x,y,y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd21f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f1450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(back[0]), len(back[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd45f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "back[0][0].shape, back[0][1].shape, back[0][2].shape, back[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4731a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "back[1][0].shape, back[1][1].shape, back[1][2].shape, back[1][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "for i in range(len(net.weights)):\n",
    "    net.weights[i] = net.weights[i] - learning_rate*back[0][i]\n",
    "    net.biases[i] = net.biases[i] - learning_rate*back[1][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f481ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/len(y)*np.sum((pred - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ededf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618df207",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(net.a_states), len(net.h_states), len(net.weights), len(net.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee69cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.weights[-2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27459cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0]).reshape(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc0044",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.a_states[-2].shape, net.a_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75785000",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.weights[-1].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f70885",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y).astype(float).reshape(32,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e17b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred-np.array(y).astype(float).reshape(32,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c13bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(net.a_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in net.a_states:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in net.weights:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9cfcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dn = np.sum(net.a_states[-2]*(pred - y),axis=0).reshape(64,1)\n",
    "deln = pred - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5fbc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "deln.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20902fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd07ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dbn = np.sum((pred - y),axis=0).reshape(1,1)\n",
    "delbn = Dbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(delbn, net.relu(net.h_states[2],derivative=True)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a9c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dn.shape, deln.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_layer = np.multiply(np.dot(deln,net.weights[2]), net.relu(net.h_states[2],derivative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_layer.T.shape, net.weights[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128794fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = []\n",
    "for col_vector in d_layer.T:\n",
    "    req.append(col_vector.T*net.a_states[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc9518",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(req).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e8e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279bb9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b58fce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035642ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.h_states[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.a_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89872932",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.weights[-1] - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "(net.a_states[-1]*(pred - np.array(train_target[:32]).astype(float).reshape(32,1))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.dtype, np.array(train_target[:32]).astype(float).reshape(32,1).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((self.pred - np.array(y).astype(float).reshape(len(y),1)),axis=0).reshape(len(self.biases[-2]),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_target[:32]\n",
    "(pred - np.array(y).astype(float).reshape(len(y),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde05e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7429bf78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32cb0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce4bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, dev_input, dev_target, test_input = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e1254",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input[0:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea86ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(train_target), max(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c442dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"https://raw.githubusercontent.com/sahasrarjn/cs725-2022-assignment/main/classification/data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b4fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687c7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
